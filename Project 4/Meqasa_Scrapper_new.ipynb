{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Information from Property Rent Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def house_info(non_ad_houses):\n",
    "    prop_features = ['bed','shower','']\n",
    "    house_urls  = []\n",
    "    house_names = []\n",
    "    house_price = []\n",
    "    house_beds  = []\n",
    "    house_showers = []\n",
    "    house_garages = []\n",
    "    house_area = []\n",
    "    #house_description\n",
    "    house_currency = []\n",
    "    house_rent_period = []\n",
    "    house_address = []\n",
    "    #house_time_posted = []\n",
    "    for link in non_ad_houses:\n",
    "        house_names.append(link.text.split(',')[0].title().strip())\n",
    "        house_urls.append(base_url + link.find('a').attrs['href'])\n",
    "        house_address.append(link.text.split(',')[0].title().split('At ')[-1])\n",
    "        price = link.find('p', {'class':'h3'}).text.replace('\\n','').replace('Price','').replace('$','').replace('GHâ‚µ','').split('/')[0].strip()\n",
    "        currency_temp = link.find('p', {'class':'h3'}).text.replace('\\n','').replace('Price','').split('/')[0].strip()\n",
    "        currency = currency_temp[0] if currency_temp[0] is '$' else 'GHC'\n",
    "        beds =   link.find('li', {'class':'bed'}).text if link.find('li', {'class':'bed'}) is not None else ''\n",
    "        showers =   link.find('li', {'class':'shower'}).text if link.find('li', {'class':'shower'}) is not None else ''\n",
    "        area = link.find('li', {'title':'Area'}).text if link.find('li', {'title':'Area'}) is not None else ''\n",
    "        garages = link.find('li', {'title':'Garages'}).text if link.find('li', {'title':'Garages'}) is not None else ''\n",
    "        period =  link.find('p', {'class':'h3'}).text.split('/ ')[-1].strip()\n",
    "        \n",
    "              \n",
    "        house_beds.append(beds)\n",
    "        house_showers.append(showers)\n",
    "        house_price.append(price)\n",
    "        house_currency.append(currency)\n",
    "        house_garages.append(garages)\n",
    "        house_area.append(area)\n",
    "        house_rent_period.append(period)\n",
    "    \n",
    "    df_houses = pd.DataFrame({'Property': house_names, \n",
    "                              'Beds': house_beds,\n",
    "                              'Currency' : house_currency,\n",
    "                              'price': house_price, \n",
    "                              'garages': house_garages, \n",
    "                              'Link': house_urls,\n",
    "                              'address': house_address,\n",
    "                              'rent_period': house_rent_period, \n",
    "                              'area': house_area,\n",
    "                              'address': house_address\n",
    "                              })\n",
    "    \n",
    "    return(df_houses)\n",
    "\n",
    "\n",
    "def next_page(page_num, property_url, requests):\n",
    "    next_url = property_url + '?w=' + str(page_num)\n",
    "    result = requests.get(next_url)\n",
    "    if result.status_code == 200:\n",
    "        print('Scraping page number:', page_num)\n",
    "    else:\n",
    "        print('Error occured in page load')\n",
    "        \n",
    "    return result, requests, next_url\n",
    "\n",
    "\n",
    "\n",
    "def scrape_newest_listing(df_saved_listings):\n",
    "    current_page = requests.get('https://meqasa.com/properties-for-rent-in-ghana')\n",
    "    src = current_page.content\n",
    "    soup = BeautifulSoup(src, 'lxml')\n",
    "    \n",
    "    non_ad_houses  = soup.findAll('div', {'class': 'mqs-prop-dt-wrapper'})\n",
    "    df_non_ad_house = house_info(non_ad_houses)\n",
    "    \n",
    "\n",
    "def save_data(df):\n",
    "    file_time = datetime.datetime.now()\n",
    "    file_time = file_time.strftime('%Y-%m-%d')\n",
    "    fine_name = 'meqasa_'+file_time+'.csv'\n",
    "    \n",
    "    df.to_csv('data/'+fine_name, index=False)\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================\n",
      "==== Scrape Meqasa ====\n",
      "Scraping page number: 1\n",
      "Scraping page number: 2\n",
      "Now gathering additional information (Broker and Listing Time)...\n",
      "\n",
      "\n",
      "Scaping Complete\n"
     ]
    }
   ],
   "source": [
    "df_final = pd.DataFrame([])\n",
    "df_first_ad = pd.DataFrame([])\n",
    "\n",
    "base_url = 'https://meqasa.com'\n",
    "result   = requests.get('https://meqasa.com/properties-for-rent-in-ghana')\n",
    "property_url = result.url\n",
    "\n",
    "# Enter the number of pages you would like to scrape\n",
    "number_of_pages = 2\n",
    "\n",
    "print('========================')\n",
    "print('==== Scrape Meqasa ====')\n",
    "\n",
    "for i in range(1, number_of_pages+1):\n",
    "    df_first_ad = pd.DataFrame([])\n",
    "    \n",
    "    result, requests, next_url = next_page(i, property_url, requests)\n",
    "    src  = result.content\n",
    "    soup = BeautifulSoup(src, 'lxml')\n",
    "    \n",
    "    #first_ad_house = soup.findAll('div', {'class': 'mqs-prop-dt-wrapper'})\n",
    "    non_ad_houses  = soup.findAll('div', {'class': 'mqs-prop-dt-wrapper'})\n",
    "    \n",
    "    #df_first_ad = house_info(first_ad_house)\n",
    "    df = house_info(non_ad_houses)\n",
    "    \n",
    "    df_final = df_final.append(df)\n",
    "  \n",
    "    \n",
    "    df_final.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "\n",
    "\n",
    "# Add the listing time and listing broker\n",
    "print('Now gathering additional information (Broker and Listing Time)...')\n",
    "\n",
    "#Newest_listing_time = df_final['Listing Time'][0]\n",
    "\n",
    "print('\\n')\n",
    "#print('Time of last listing posted:', Newest_listing_time)\n",
    "    \n",
    "save_data(df_final)\n",
    "print('Scaping Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
